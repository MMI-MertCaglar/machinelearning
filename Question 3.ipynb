{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convolutional Neural Networks (60 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ümit Mert ÇAĞLAR 2043685"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the convolutional neural network shown below for CIFAR-10 dataset.  Your code must follow these rules:\n",
    "\n",
    "__- Use your own implementation.__\n",
    "\n",
    "__- You can only use \" tf \" and \" tf.nn \" libraries. Check [tf.nn](https://www.tensorflow.org/api_docs/python/tf/nn)__\n",
    "\n",
    "__- Use ReLU activation function__\n",
    "\n",
    "__- Use batch normalization for convolutional layers__\n",
    "\n",
    "__- Use dropout for fully connected layers (for training only)__\n",
    "\n",
    "__- Write necessary explanations for each cell. Explanations should be detailed.__\n",
    "\n",
    "__- You can use codes from Lab Notebook.__\n",
    "\n",
    "__- (OPTIONAL) You can add more layers or use different methods for better accuracy. If you want, send an another notebook file with better accuracy for bonus points.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "You can normalize input data \"x\" if needed. You can use \"np.squeeze\" to remove single-dimensional entries from the shape of an array. You probably need to convert labels \"y\" to one-hot vector for loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary for the classes (as defined in cifar-10 website) are defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict={0 : 'airplane',\n",
    "            1 : 'automobile',\n",
    "            2 : 'bird',\n",
    "            3 : 'cat',\n",
    "            4 : 'deer',\n",
    "            5 : 'dog',\n",
    "            6 : 'frog',\n",
    "            7 : 'horse',\n",
    "            8 : 'ship',\n",
    "            9 : 'truck',\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 : airplane\n",
    "1 : automobile\n",
    "2 : bird\n",
    "3 : cat\n",
    "4 : deer\n",
    "5 : dog\n",
    "6 : frog\n",
    "7 : horse\n",
    "8 : ship\n",
    "9 : truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 airplane\n",
      "1 automobile\n",
      "2 bird\n",
      "3 cat\n",
      "4 deer\n",
      "5 dog\n",
      "6 frog\n",
      "7 horse\n",
      "8 ship\n",
      "9 truck\n"
     ]
    }
   ],
   "source": [
    "for i in class_dict:\n",
    "    print(str(i)+\" \"+class_dict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems correct. Let's observe a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       ...,\n",
       "       [9],\n",
       "       [1],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train image of : 14378 is an image of a automobile\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x272675aa0b8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample=np.random.randint(0,len(x_train))\n",
    "print(\"x_train image of : \"+str(sample)+\" is an image of a \"+str(class_dict[y_train[sample][0]]))\n",
    "plt.imshow(x_train[sample]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test image of : 8419 is an image of a horse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27274abad30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample=np.random.randint(0,len(x_test))\n",
    "print(\"x_test image of : \"+str(sample)+\" is an image of a \"+str(class_dict[y_test[sample][0]]))\n",
    "plt.imshow(x_test[sample]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image changes everytime randomly each time above code is run. But all images I checked were correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets first observe what kind of input we have and output we desire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape # (50000, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 50000 of 32x32 images in 3 channels (RGB) in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape #(10000, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10000 of 32x32 images in 3 channels (RGB) in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape # original (50000,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train and y_test can be converted into an array because 1 dimension of 1 element means it is an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=y_train.reshape(-1,) # -1 means we state unknown length of first dimension (stays same)\n",
    "y_train.shape #(50000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=y_test.reshape(-1,)\n",
    "y_test.shape # (10000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train and y_test are arrays for the labels of the training and test sets of x_train and x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of y is: 0\n",
      "Maximum value of y is: 9\n",
      "The number of classes is: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum value of y is: \"+str(min(y_train))+\n",
    "      \"\\nMaximum value of y is: \"+str(max(y_train))+\n",
    "     \"\\nThe number of classes is: \"+ str(max(y_train)-min(y_train)+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is used to define values ranging from 0 to 9 in this array with a matrix where we have 1 for the corresponding class and 0 for all other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 10 # 10 classes\n",
    "y_test_one_hot=tf.Session().run(tf.one_hot(y_test, depth,on_value=1,off_value=0)).reshape(-1,depth)\n",
    "y_train_one_hot=tf.Session().run(tf.one_hot(y_train, depth,on_value=1,off_value=0)).reshape(-1,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train set with one hot encoding: (50000, 10)\n",
      "Shape of y_test  set with one hot encoding: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of y_train set with one hot encoding: \"+str(y_train_one_hot.shape))\n",
    "print(\"Shape of y_test  set with one hot encoding: \"+str(y_test_one_hot.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train set: (50000, 32, 32, 3)\n",
      "Shape of x_test  set: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of x_train set: \"+str(x_train.shape))\n",
    "print(\"Shape of x_test  set: \"+str(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define placeholders we have to decide on the size of input images. From the CNN defined in the question we have 32x32@3 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape and Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dimensions can also be given explicitly if we want to have a generic solution that does not depend on the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that CIFAR-10 images are 32 pixels in each dimension.\n",
    "img_size = 32\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 10\n",
    "full_layer_size=128 #predefine fully connected layer neuron number\n",
    "channel_depth=3 #number of channels\n",
    "img_size_flat=img_size*img_size*channel_depth #32x32@3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to keep the number of images in one dimension and 32x32@3=3072 pixels in one dimension, for this we will reshape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat=x_train.reshape(-1,img_size_flat)\n",
    "x_test_flat=x_test.reshape(-1,img_size_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_flat.shape)\n",
    "print(x_test_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we might want to keep images in widthxlength@channel format as we will use conv2d which is 2dimensional convolutional operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define placeholders\n",
    "\n",
    "Define input and output placeholders. It is a good idea to pass dropout rate also as a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_train_input = tf.placeholder(tf.float32, [None, img_size,img_size,channel_depth])\n",
    "print(x_train_input)\n",
    "x_test_input = tf.placeholder(tf.float32, [None, img_size,img_size,channel_depth])\n",
    "print(x_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_2:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"Placeholder_3:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_train_output =  tf.placeholder(tf.float32, [None, num_classes])\n",
    "print(y_train_output)\n",
    "y_test_output =  tf.placeholder(tf.float32, [None, num_classes])\n",
    "print(y_test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My initial thought was to define all train and test inputs and outputs as placeholders, however as their size are not dependent defining x for all inputs and y for all outputs was enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_4:0\", shape=(?, 3072), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, img_size_flat])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image_size_flat corresponds to the fully connected layer which is used for classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_5:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"Placeholder_6:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.placeholder(tf.float32, [None, num_classes])\n",
    "y_true_cls = tf.placeholder(tf.int64, [None])\n",
    "print(y_true)\n",
    "print(y_true_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True predictions and true classes are passed as placeholders, our tensor graph should be able to link inputs to their corresponding outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits=tf.placeholder(tf.float32, [None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logits is the basic output definition, it is basically defined as y=x*weights+biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=tf.placeholder(tf.float32, [None,num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output has the same shape and format as logits, basically output is the probability of an image being a class.\n",
    "Note that we are using softmax which equates the sum of all possibilities to 1, which is mathematically adaquate to assume the corresponding float values are probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables\n",
    "\n",
    "Define filters for convolutional layers, weights and biases for fully connected layers. Don't forget to use an initializer. Xavier is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "weights = tf.Variable(tf.zeros([img_size_flat, num_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining weights and biases as well as convolutional layers as variables. Convolutional layer outputs are defined as variables here, however they are not again used succesfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = tf.Variable(tf.zeros([num_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_2:0' shape=(32, 32, 3) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_3:0' shape=(128, 10) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "Conv_layer_1=tf.Variable(tf.zeros([img_size,img_size,channel_depth]))\n",
    "fully_connected=tf.Variable(tf.zeros([full_layer_size,num_classes]))\n",
    "print(Conv_layer_1)\n",
    "print(fully_connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(x, weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network\n",
    "\n",
    "Define the network as a function.Recommended functions are:\n",
    "\n",
    "`\n",
    "tf.nn.convolution / tf.nn.pool / tf.nn.batch_normalization / tf.nn.dropout / tf.reshape`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model.png\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# input\n",
    "input_layer=tf.Variable(tf.zeros([img_size,img_size,channel_depth]))\n",
    "# first Convolutional layer\n",
    "layer2=tf.nn.convolution(input=input_layer,padding='SAME',filter=[3,3,32,32])\n",
    "layer2=tf.nn.relu(layer2) #relu activation function applied to layer2's output\n",
    "# first Pooling layer\n",
    "layer3=tf.nn.pool(layer2,\n",
    "    window_shape=[5,5],\n",
    "    pooling_type='MAX',\n",
    "    padding='SAME',\n",
    "    strides=[2,2])\n",
    "# Second Convolutional layer\n",
    "layer4=tf.nn.convolution(input=layer3,padding='SAME',filter=[3,3,16,16])\n",
    "layer4=tf.nn.relu(layer4)\n",
    "# Second Pooling layer\n",
    "layer5=tf.nn.pool(layer4,\n",
    "    window_shape=[5,5],\n",
    "    pooling_type='MAX',\n",
    "    padding='SAME',\n",
    "    strides=[1,1])\n",
    "# Third Convolutional layer\n",
    "layer6=tf.nn.convolution(input=layer5,padding='SAME',filter=[3,3,16,32])\n",
    "layer6=tf.nn.relu(layer6)\n",
    "# Third Pooling layer\n",
    "layer7=tf.nn.pool(layer6,\n",
    "    window_shape=[3,3],\n",
    "    pooling_type='MAX',\n",
    "    padding='SAME',\n",
    "    strides=[2,2])\n",
    "# Fully Connected Layer\n",
    "logits = tf.matmul(layer7.reshape(-1,), weights) + biases\n",
    "#Softmax\n",
    "y_pred = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell gives errors in tensorflow, I could not debug the errors but matrix dimensions and their connections should be correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=int64>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logits = tf.matmul(x, weights) + biases\n",
    "y_true_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ArgMax:0' shape=(?,) dtype=int64>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define cost function\n",
    "\n",
    "Define cost with respect to predictions and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                           labels=y_true_cls)\n",
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of completing this training run with the lab1(assignment1) code I choose the same parameters as they were in lab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimizer\n",
    "\n",
    "Adam is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientDescentOptimizer with fixed learning rate was tried at first, due to training errors I did not move to adaptive gradient descent optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define performance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a few more performance measures to display the progress to the user.\n",
    "\n",
    "This is a vector of booleans whether the predicted class equals the true class of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculates the classification accuracy by first type-casting the vector of booleans to floats, so that False becomes 0 and True becomes 1, and then calculating the average of these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiates a tensorflow session, the output of this session are typical numpy elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize variables\n",
    "\n",
    "The variables for `weights`,`filters` and `biases` must be initialized before we start optimizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train function\n",
    "\n",
    "Don't forget to use batches since dataset is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint : One of the simplest way to define batch is\n",
    "\n",
    "`for b in range(dataset_size//batch_size):      \n",
    "    x_batch = x_train[b * batch_size : (b+1) * batch_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "dataset_size=x_train.shape[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for b in range(int(dataset_size/batch_size)):      \n",
    "    x_batch = x_train[b*batch_size:(b+1)*batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for i in range(epoch):\n",
    "        for b in range(int(dataset_size/batch_size)): \n",
    "            # Get a batch of training examples.\n",
    "            # x_batch now holds a batch of images and\n",
    "            # y_true_batch are the true labels for those images.\n",
    "            x_batch=x_train_flat[b*batch_size:(b+1)*batch_size]\n",
    "            y_true_batch=y_train_one_hot[b*batch_size:(b+1)*batch_size]\n",
    "            #x_batch, y_true_batch = data.train.next_batch(batch_size)\n",
    "            # Put the batch into a dict with the proper names\n",
    "            # for placeholder variables in the TensorFlow graph.\n",
    "            # Note that the placeholder for y_true_cls is not set\n",
    "            # because it is not used during training.\n",
    "            feed_dict_train = {x: x_batch,\n",
    "                               y_true: y_true_batch}\n",
    "\n",
    "            # Run the optimizer using this batch of training data.\n",
    "            # TensorFlow assigns the variables in feed_dict_train\n",
    "            # to the placeholder variables and then runs the optimizer.\n",
    "            session.run(optimizer, feed_dict=feed_dict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 9, ..., 9, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dict with the test-set data to be used as input to the TensorFlow graph. Note that we must use the correct names for the placeholder variables in the TensorFlow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_test = {x: x_test_flat, # input is the flat array of images\n",
    "                  y_true: y_test_one_hot, # true output is the matrix of one hot encoded y_test\n",
    "                  y_true_cls: y_test} #corresponding class values are given in original y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=int64>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed_dict_test is compared with lab1, they are identical yet my test feed dictionary gives some error I could not fix/debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<tf.Tensor 'Placeholder_4:0' shape=(?, 3072) dtype=float32>: array([[158, 112,  49, ...,  21,  67, 110],\n",
       "        [235, 235, 235, ..., 186, 200, 199],\n",
       "        [158, 190, 222, ...,   7,   8,   7],\n",
       "        ...,\n",
       "        [ 20,  15,  12, ...,  25,  20,  47],\n",
       "        [ 25,  40,  12, ...,  92, 120,  80],\n",
       "        [ 73,  78,  75, ...,  27,  26,  26]], dtype=uint8),\n",
       " <tf.Tensor 'Placeholder_5:0' shape=(?, 10) dtype=float32>: array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 1, 0],\n",
       "        [0, 0, 0, ..., 0, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 1, 0, 0]]),\n",
       " <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=int64>: array([3, 8, 8, ..., 5, 1, 7])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ArgMax:0' shape=(?,) dtype=int64>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance before training\n",
    "\n",
    "The accuracy is expected to be around 10%\n",
    "\n",
    "Peformance metrics are defined below in functions for printing accuracy and getting it as well as plotting weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy():\n",
    "    \n",
    "    # Run the model to get predictions for test data\n",
    "    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
    "    \n",
    "    # Get true labels\n",
    "    cls_true = y_test# data.test.cls\n",
    "    \n",
    "    # Calculate the difference betweeb predictions and true labels\n",
    "    correct_prediction = np.equal(cls_pred, cls_true)\n",
    "    \n",
    "    # Calculate the total accuracy\n",
    "    acc = np.mean(correct_prediction)\n",
    "\n",
    "    # Print the accuracy.\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy():\n",
    "    \n",
    "    # Run the model to get predictions for test data\n",
    "    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
    "    \n",
    "    # Get true labels\n",
    "    cls_true = y_test# data.test.cls\n",
    "    \n",
    "    # Calculate the difference betweeb predictions and true labels\n",
    "    correct_prediction = np.equal(cls_pred, cls_true)\n",
    "    \n",
    "    # Calculate the total accuracy\n",
    "    acc = np.mean(correct_prediction)\n",
    "\n",
    "    # Print the accuracy.\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights():\n",
    "    # Get the values for the weights from the TensorFlow variable.\n",
    "    w = session.run(weights)\n",
    "    \n",
    "    # Get the lowest and highest values for the weights.\n",
    "    # This is used to correct the colour intensity across\n",
    "    # the images so they can be compared with each other.\n",
    "    w_min = np.min(w)\n",
    "    w_max = np.max(w)\n",
    "\n",
    "    # Create figure with 3x4 sub-plots,\n",
    "    # where the last 2 sub-plots are unused.\n",
    "    fig, axes = plt.subplots(3, 4)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only use the weights for the first 10 sub-plots.\n",
    "        if i<10:\n",
    "            # Get the weights for the i'th digit and reshape it.\n",
    "            # Note that w.shape == (img_size_flat, 10)\n",
    "            image = w[:, i].reshape(img_shape)\n",
    "\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"Weights: {0}\".format(i))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(image, vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "\n",
    "        # Remove ticks from each sub-plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 10.0%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected accuracy on test set is 10% which corresponds to 5000 of images being 0 (airplane)in the dataset as we have initialized as all zeros 10% are correctly labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance after training\n",
    "\n",
    "Measure training and test accuracy for at least 10 epochs, show it on a epoch/accuracy graph. The network is expected to reach around 70% accuracy at 10 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_6' with dtype int64 and shape [?]\n\t [[node Placeholder_6 (defined at <ipython-input-26-f21d8bac7c98>:2) ]]\n\nCaused by op 'Placeholder_6', defined at:\n  File \"C:\\Users\\M\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\asyncio\\base_events.py\", line 528, in run_forever\n    self._run_once()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1764, in _run_once\n    handle._run()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1080, in __init__\n    self.run()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-f21d8bac7c98>\", line 2, in <module>\n    y_true_cls = tf.placeholder(tf.int64, [None])\n  File \"C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2077, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6834, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_6' with dtype int64 and shape [?]\n\t [[node Placeholder_6 (defined at <ipython-input-26-f21d8bac7c98>:2) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_6' with dtype int64 and shape [?]\n\t [[{{node Placeholder_6}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-991f7a0c2d02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-47-5b4764b6bc76>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;31m# TensorFlow assigns the variables in feed_dict_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# to the placeholder variables and then runs the optimizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_6' with dtype int64 and shape [?]\n\t [[node Placeholder_6 (defined at <ipython-input-26-f21d8bac7c98>:2) ]]\n\nCaused by op 'Placeholder_6', defined at:\n  File \"C:\\Users\\M\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\asyncio\\base_events.py\", line 528, in run_forever\n    self._run_once()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1764, in _run_once\n    handle._run()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1080, in __init__\n    self.run()\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\M\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-f21d8bac7c98>\", line 2, in <module>\n    y_true_cls = tf.placeholder(tf.int64, [None])\n  File \"C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2077, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6834, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_6' with dtype int64 and shape [?]\n\t [[node Placeholder_6 (defined at <ipython-input-26-f21d8bac7c98>:2) ]]\n"
     ]
    }
   ],
   "source": [
    "train(epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder_6' with dtype int64 and shape [?]\n",
    "\t [[node Placeholder_6 (defined at <ipython-input-26-f21d8bac7c98>:2) ]]\n",
    "        \n",
    "        InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_6' with dtype int64 and shape [?]\n",
    "\t [[node Placeholder_6 (defined at <ipython-input-26-f21d8bac7c98>:2) ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training gives the above error, I checked nearly all variables and placeholders as they were in lab1, everything seems identical/similar yet I still get this error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 10.0%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could not be able to debug/fix my code from the errors. From theoretical aspect I can deduce some discussions:\n",
    "Convolutional Neural Networks (CNNs) are used for automatical filter generation. This term might sound generic but CNNs are very successful at imitating handcrafted science related filters. Instead of scientists/engineers CNNs map their Inputs to the outputs with the help of backpropagation and long training time where CNN is exposed to data.\n",
    "\n",
    "#### Keynotes and their explanations used in this notebook:\n",
    "\n",
    "###### Rectified Linear Unit: \n",
    "is the nonlinearity that helped Deep Learning shine. Basically with the help of these units (and with many of them) we can fit a formula for a complex (nonlinear) problem.\n",
    "###### batch normalization: \n",
    "Improves performance and stability of neural networks, helps optimization. Normalizes inputs of each layer to have zero mean and standard deviatiof of 1 in output activation. Widely used to combat internal covariate shift. Used after filtering and before activation function (RELU). Regularizes the modeli reduces need for dropout.\n",
    "###### Dropout:\n",
    "Combining predictions is a strong method but it is costy. So drop each neuron in the network with a probability p (generally with p=0.5 50% of the time). This way each neuron acts independently and does not rely solely on another neuron. Forces neurons to learn robust features in conjuction with each other. Dropout is only used in training, in test all neurons are available.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I have found the below code parts from "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of filters.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=True):  # Use 2x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    # This format is determined by the TensorFlow API.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = new_weights(shape=shape)\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    # Note the strides are set to 1 in all dimensions.\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel.\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image.\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of filters.\n",
    "                   num_filters):\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "    biases =  tf.Variable(tf.constant(0.05, shape=[num_filters]))\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "    layer += biases\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer, weights\n",
    "\n",
    "def pool_layer(input,              # The previous layer.\n",
    "               num_input_channels, # Num. channels in prev. layer.\n",
    "               filter_size,        # Width and height of filters.\n",
    "               num_filters):\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "    biases =  tf.Variable(tf.constant(0.05, shape=[num_filters]))\n",
    "    layer += biases\n",
    "    layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "WARNING:tensorflow:From C:\\Users\\M\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Using real-time data augmentation.\n",
      "WARNING:tensorflow:From C:\\Users\\M\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n\t [[{{node metrics/acc/Mean}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-5b30fe9afa36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                         workers=4)\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;31m# Save model and weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n\t [[{{node metrics/acc/Mean}}]]"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "steps_per_epoch=100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        steps_per_epoch=100,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't know how but even the keras's CIFAR-10 code I directly copied for reference did not work :)\n",
    "\n",
    "UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n",
    "\t [[{{node conv2d_1/convolution}}]]\n",
    "\t [[{{node metrics/acc/Mean}}]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] [Arm Community](https://community.arm.com)\n",
    "\n",
    "[2] [Keras Cifar-10 Example](https://keras.io/examples/cifar10_cnn/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
